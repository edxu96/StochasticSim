
## Multiple Linear Regression {#multi}

### Standardization




### Multicollinearity Diagnosis {#multi-diag}

> In some situations the regressors are nearly perfectly linearly related, and in such cases the inferences based on the regression model can be misleading or erroneous. When there are near-linear dependencies among the regressors, the problem of multicollinearity is said to exist. [9, @montgomery2012introduction]

There are four primary sources of multicollinearity: [9, @montgomery2012introduction]

1. The data collection method employed
2. Constraints on the model or in the population
3. Model specification
4. An overdefined model

> To really establish causation, it is usually necessary to do an experiment in which the putative causative variable is manipulated to see what effect it has on the response. [1.5.7, @wood2017generalized]

#### (1) Covariance Matrix {-}

> Inspection of the covariance matrix is not sufficient for detecting anything more complex than pair- wise multicollinearity. [9.4.1 Examination of the Correlation Matrix, @montgomery2012introduction]

```{example, echo = TRUE}
It can be seen from the following covariance matrix that `y` is highly correlated to `x2`, `x6` and `x8`. Besides, `x3`-`x4`, `x2`-`x6`, `x4`-`x8` are high correlated.
```

```{r, fig.cap='(ref:multi-1)'}
dat %>%
  cor() %>%
  corrplot::corrplot(type = "upper", tl.col = "black", tl.srt = 45)
```

(ref:multi-1) Heat map for the covariance matrix of `recs`.

*Variance Inflation Factors (VIF)*

The collinearity diagnostics in R require the packages “perturb” and “car”. The R code to generate the collinearity diagnostics for the delivery data is:

```r
deliver.model <- lm(time∼cases+dist, data=deliver)
print(vif(deliver.model))
print(colldiag(deliver.model))
```

### Orthogonalization

*****
Regress `y` on `x2`, Assumptions and Orthogonalization

`mods[[1]]` is obtained by regressing `y` on `x2`.

```{r}
mods <- list()
mods[[1]] <- lm(y ~ x2, data = dat)

mods[[1]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)

results <- new_results(mods[[1]], 1)
```

By orthogonalizing `x2` with respect to constant 1. the following reparameterized model can be obtained.

```{r, echo=T}
mods[[2]] <-
  dat %>%
  mutate(x1 = 1, x21 = x2 - mean(.$x2)) %>%
  dplyr::select(y, x1, x21) %>%
  {lm(y ~ x1 + x21, data = .)}
```

```{r}
results %<>%
  collect_glance(mods[[2]], 2)

mods[[2]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

The estimated regression coefficient for `x2` in `mods[[1]]` equals that for `x21` in `mods[[2]]`. That is, slopes in these two models are the same. The standard error of the intercept is reduced by 51.60 %.

*****
