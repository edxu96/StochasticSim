---
editor_options:
  chunk_output_type: console
---

## Multiple Linear Regression {#multi}

### Standardisation




### Multicollinearity Diagnosis {#multi-diag}

> In some situations the regressors are nearly perfectly linearly related, and in such cases the inferences based on the regression model can be misleading or erroneous. When there are near-linear dependencies among the regressors, the problem of multicollinearity is said to exist. -- chapter 9 in [@montgomery2012introduction]

There are four primary sources of multicollinearity: -- chapter 9 in [@montgomery2012introduction]

1. The data collection method employed
2. Constraints on the model or in the population
3. Model specification
4. An overdefined model

> To really establish causation, it is usually necessary to do an experiment in which the putative causative variable is manipulated to see what effect it has on the response. -- section 1.5.7 in [@wood2017generalized :]

#### (1) Covariance Matrix {-}

> Inspection of the covariance matrix is not sufficient for detecting anything more complex than pair- wise multicollinearity. -- section 9.4.1 Examination of the Correlation Matrix in [@montgomery2012introduction]

For example, it can be seen from the following covariance matrix that `y` is highly correlated to `x2`, `x6` and `x8`. Besides, `x3`-`x4`, `x2`-`x6`, `x4`-`x8` are high correlated.

```{r, fig.cap='(ref:multi-1)'}
dat_recs %>%
  cor() %>%
  corrplot::corrplot(type = "upper", tl.col = "black", tl.srt = 45)
```

(ref:multi-1) Heat map for the covariance matrix of `recs`.

#### (2) Variance Inflation Factors (VIF) {-}

The collinearity diagnostics in R require the packages “perturb” and “car”. The R code to generate the collinearity diagnostics for the delivery data is:

```{r}
mods_delivery <- list()
mods_delivery[[1]] <- lm(time ~ case + dist, data = dat_delivery)
mods_delivery[[1]] %>% car::vif()
mods_delivery[[1]] %>% perturb::colldiag()
```

```{r}
mods_recs[[1]] %>% car::vif()
mods_recs[[1]] %>% perturb::colldiag()
```

### Orthogonalisation

> This will facilitate solving the likelihood equations, and also help the general interpretation and use of regression models. [@hendry2007econometric]

> Orthogonalizing the regressors does not change the model and the likelihood, but it eliminates the near collinearity. [@hendry2007econometric]

> In contrast to this situation, perfect collinearity is unique to the model and cannot be eliminated by orthogonalization. [@hendry2007econometric]

- [Orthogonalized regression reference?](https://stats.stackexchange.com/a/458558/231405)

For example, regressors in `mods_recs[[4]]` can be orthogonalised to have a better model `mods_recs[[5]]`.

```{r}
mods_recs[[5]]$call
mods_recs[[5]] %>% tidy() %>% tab_ti()
mods_recs[[5]] %>% glance() %>% tab_ti()
```

> The intercept parameter would have a more reasonable interpretation in a reparametrized model. [@hendry2007econometric]

The intercept (8.36) in `mods_recs[[5]]` can be interpreted as the exptected value for an individual with average values of `x2`, `x8` and `I(x2^2)`, which can be verified by the prediction using `mods_recs[[4]]`.

```{r, echo=T}
( 8.75527 - 0.51392 * mean(dat_recs$x2) + 0.07631 * mean(dat_recs$x8) +
  0.03271 * mean(dat_recs$x2^2) ) - 8.35989 <= 1e-4
```

Since the standard errors in `mods_recs[[5]]` are smaller than those in `mods_recs[[4]]`, `mods_recs[[5]]` is used to conduct inference. Particularly, `se` for `Intercept` is reduced by 75.48%, and `se`s for first two regressors are reduced by 71.75% and 2.41%. The estimations for the last term are exactly the same, which is expected.
