---
editor_options:
  chunk_output_type: console
---

## Multiple Linear Regression {#multi}

### Standardization




### Multicollinearity Diagnosis {#multi-diag}

> In some situations the regressors are nearly perfectly linearly related, and in such cases the inferences based on the regression model can be misleading or erroneous. When there are near-linear dependencies among the regressors, the problem of multicollinearity is said to exist. [9, @montgomery2012introduction]

There are four primary sources of multicollinearity: [9, @montgomery2012introduction]

1. The data collection method employed
2. Constraints on the model or in the population
3. Model specification
4. An overdefined model

> To really establish causation, it is usually necessary to do an experiment in which the putative causative variable is manipulated to see what effect it has on the response. [@wood2017generalized :1.5.7]

#### (1) Covariance Matrix {-}

> Inspection of the covariance matrix is not sufficient for detecting anything more complex than pair- wise multicollinearity. [@montgomery2012introduction :9.4.1 Examination of the Correlation Matrix]

```{example, echo=T}
It can be seen from the following covariance matrix that `y` is highly correlated to `x2`, `x6` and `x8`. Besides, `x3`-`x4`, `x2`-`x6`, `x4`-`x8` are high correlated.
```

```{r, fig.cap='(ref:multi-1)'}
dat_recs %>%
  cor() %>%
  corrplot::corrplot(type = "upper", tl.col = "black", tl.srt = 45)
```

(ref:multi-1) Heat map for the covariance matrix of `recs`.

#### (2) Variance Inflation Factors (VIF) {-}

The collinearity diagnostics in R require the packages “perturb” and “car”. The R code to generate the collinearity diagnostics for the delivery data is:

```{r}
mods_delivery <- list()
mods_delivery[[1]] <- lm(time ~ case + dist, data = dat_delivery)
mods_delivery[[1]] %>% car::vif()
mods_delivery[[1]] %>% perturb::colldiag()
```

```{r}
mods_recs[[1]] %>% car::vif()
mods_recs[[1]] %>% perturb::colldiag()
```

### Orthogonalization
