## 7. Models with More Regressors

### 7-1. Benchmark Model

`mods[[3]]` with `x2` and `x6` being regressors is a good model already and pass every test. It is chosen as the benchmark model after the discussion in subsection 7-4.

```{r}
mods[[3]] <- lm(y ~ x2 + x6, data = dat_2)
results %<>% collect_glance(mods[[3]], 3)

## Print model summary
model <- mods[[3]]
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

```{r, echo=T}
results_test <- mods[[3]] %>% test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[3]], dat_2, resi2 ~ x2 + x6 + I(x2^2) + I(x6^2),
    3)) %>%
  bind_rows(test_white(mods[[3]], dat_2, resi2 ~ x2 + x6 + I(x2^2) + I(x6^2) +
    I(x2 * x6), 6)) %>%
  bind_rows(test_reset(mods[[3]], dat_2))

results_test %>% tab_ti(F)
```

### 7-2. Model with All Regressors

To construct `mods[[4]]`, all of the variables (excluding `x8`) are included. `mods[[4]]` can pass Jarque-Bera test and two White's tests, but cannot pass  RESET test.

```{r}
mods[[4]] <- lm(y ~ x2 + x3 + x4 + x5 + x6 + x7, data = dat_2)
results %<>% collect_glance(mods[[4]], 4)
mods[[4]] %>% tab_tidy(T)
```

```{r, echo=T}
results_test <- mods[[4]] %>% test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[4]], dat_2, resi2 ~ x2 + x3 + x4 + x5 + x6 + x7 + 
    I(x2^2) + I(x3^2) + I(x4^2) + I(x5^2) + I(x6^2) + I(x7^2), 7)) %>%
  bind_rows(test_white(mods[[4]], dat_2, resi2 ~ x2 + x3 + x4 + x5 + x6 + x7 + 
    I(x2^2) + I(x3^2) + I(x4^2) + I(x5^2) + I(x6^2) + I(x7^2) + I(x2 * x3) +
    I(x2 * x4) + I(x2 * x5) + I(x2 * x6) + I(x2 * x7) + I(x4 * x3) + 
    I(x5 * x3) + I(x6 * x3) + I(x3 * x7) + I(x4 * x5) + I(x4 * x6) + 
    I(x4 * x7) + I(x5 * x6) + I(x5 * x7) + I(x6 * x7), 28)) %>%
  bind_rows(test_reset(mods[[4]], dat_2))

results_test %>% tab_ti(F)
```

### 7-3. Likelihood Ratio Test

### 7-4. Automated Model Selection

Thus, `mods[[5]]` is determined by automated model selection using `mods[[4]]` with `stats::step` function. `mods[[5]]` can pass Jarque-Bera test and two White's tests, but cannot pass RESET test.

```{r}
mods[[5]] <- quiet(step(mods[[4]]))
results %<>% collect_glance(mods[[5]], 5)

mods[[5]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

```{r, echo=T}
results_test <- mods[[5]] %>% test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[5]], dat_2, resi2 ~ x2 + x3 + x4 + x6 + I(x2^2) +
    I(x3^2) + I(x4^2) + I(x6^2), 5)) %>%
  bind_rows(test_white(mods[[5]], dat_2, resi2 ~ x2 + x3 + x4 + x6 + 
    I(x2^2) + I(x3^2) + I(x4^2) + I(x6^2) + I(x2 * x3) + I(x2 * x4) + 
    I(x2 * x6) + I(x4 * x3) + I(x6 * x3) +I(x4 * x6), 15)) %>%
  bind_rows(test_reset(mods[[5]], dat_2))

results_test %>% tab_ti(F)
```

According to results of five models, though `mods[[4]]` and `mods[[5]]` have lower AIC, `mods[[3]]` is the one with all tests passed and lowst AIC. It is discussed intensively in subsection 8-1, and acts as the benchmark model in section 9. Besides, `mods[[3]]` has the lowest BIC.

```{r}
results %>% tab_ti(F)
```

Additionally, `mods[[4]]` has the highest `r.squared` for including more regressors, but its `adj.r.squared` is penalized for that. The AIC and BIC are not low as well compared to those of `mods[[3]]` and `mods[[5]]`. 

## 8. Causality and Mediation

### 8-1. Causality of `x2`-`x6`

It can be seen from the following two models that `y` is highly correlated to `x2` and `x6` separately. Besides, according to `mods[[3]]`, `y` is highly correlated to `x2` and `x6` at the same time. 

```{r}
mods_62 <- list()
mods_62[[1]] <- lm(y ~ x2, data = dat_2)

mods_62[[1]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

```{r}
mods_62[[2]] <- lm(y ~ x6, data = dat_2)

mods_62[[2]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

It is reasonable to assume that people tend to have more accompanies as age increases after 18. Also, the following model proves the relationship between `x2` and `x6`. We can say that the effect of `x6` on `y` is mediated by `x2`. With `x6` affecting `y` as well, `x2` does not mediate `x6` completely. So `x2` and `x6` are both supposed to appear in the model for `y`. The mediation factor `x2` may affect `y` though other ways, which will be explored in section 10. Besides, we find that `x6` does not affect `y` in other ways in section 10.

```{r}
mods_62[[3]] <- lm(x2 ~ x6, data = dat_2)

mods_62[[3]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

### 8-2. Causality of `x3`-`x4`

When taking `x3` and `x4` into consideration, the models assocating `y` with `x3` or `x4` both show no significance, though `x3` and `x4` are highly correlated. So neither `x3` nor `x4` should be included in the model.

```{r}
mods_34 <- list()
mods_34[[1]] <- lm(x4 ~ x3, data = dat_2)

mods_34[[1]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

```{r}
mods_34[[2]] <- lm(formula = y ~ x3 + x4, data = dat_2)

mods_34[[2]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```


### 8-3. Causality of `x4`-`x8`

It can be seen that `y` is highly correlated to `x8`.

```{r}
mods_48 <- list()
mods_48[[1]] <-  lm(y ~ x8, data = dat_2)

mods_48[[1]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

Also, `x8` is highly related to `x4`, which makes sense, because people with more income tend to buy houses with more rooms.

```{r}
mods_48[[2]] <-  lm(x8 ~ x4, data = dat_2)

mods_48[[2]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

However, from subsection 9.2 we already know that `y` is not highly correlated with `x4`, which is illustrated again by the following model. So we say that the effect of `x4` on `y` is completely mediated by `x8`. Though it makes sense that people with more income tend to consume more energy on average, the direct effect is completely mediated through `x8` and possibly other factors.

```{r}
mods_48[[3]] <- lm(y ~ x8 + x4, data = dat_2)

mods_48[[3]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

## 9. `mods[[3]]` with extra terms

In `mods[[6]]`, `x2`, `x6` and `x8` are kept at the same time.

```{r}
mods[[6]] <- lm(y ~ x2 + x6 + x8, data = dat_2)

mods[[6]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

```{r}
results_test <- mods[[6]] %>% test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[6]], dat_2, resi2 ~ x2 + x6 + x8 + I(x2^2) +
    I(x6^2) + I(x8^2), 4)) %>%
  bind_rows(test_white(mods[[6]], dat_2, resi2 ~ x2 + x6 + x8 + I(x2^2) +
    I(x6^2) + I(x8^2) + I(x2 * x6) + I(x2 * x8) + I(x6 * x8), 10)) %>%
  bind_rows(test_reset(mods[[6]], dat_2))

results_test %>% tab_ti(F)
```

With the idea of diminishing effects of the number of household members in mind, it is natural to include the square of the household numbers. In addition, `x8` is included according to the discussion in subsection 8-3. Thus, `mods[[7]]` is estimated. Being insignificant, `x6` is excluded, beccause the effect of `x6` on `y` seems to be mediated compeletely by `x2` and `x2^2`.

```{r}
mods[[7]] <- lm(y ~ x2 + x8 + I(x2^2), data = dat_2)
mods[[7]] %>% tab_tidy(T)
```

With number of observations being 299, the model is sensitive to exlusion of some term only if the partial correlation of that term is smaller than 0.01276551. That is, if `p.r.squared` of some term displayed above is smaller than 0.01276551, the hypothesis regaring the coefficient for that term in the likelihood ratio test would not be rejected. The value `0.01276551` is obtained according to the discussion in subsection 7-3. Besides, small `p.value`s indicate that terms cannot be excluded as well. 

```{r, include=F}
lr_x5 <- - 299 * log(1 - 0.01276551)
(1 - pchisq(lr_x5, 1))
```

```{r}
results_test <- mods[[7]] %>% test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[7]], dat_2, resi2 ~ x2 + x8 + I(x2^2) + I(x8^2),
    3)) %>%
  bind_rows(test_white(mods[[7]], dat_2, resi2 ~ x2 + x8 + I(x2^2) +
    I(x8^2) + I(x2 * x8), 6)) %>%
  bind_rows(test_reset(mods[[7]], dat_2))

results_test %>% tab_ti(F)
```

`mods[[7]]` has a lower AIC than `mods[[6]]` and `mods[[3]]`, so it is choosen as the final model for presentation and conclusion.

```{r}
AIC(mods[[7]], mods[[6]], mods[[3]]) %>% tab_ti(F)
```

## 10. Interpretation of `mods[[7]]`

### 10-1. Model Structure

The overall structure can be illustrated in the following figure. The income is correlated to the number of household memebers, which is probably caused by the correlation between numbers of people and rooms. Because those relations don't contribute to the interpretation of consumptions directly, they are omitted.

![Illustration of the model structure.](./images/1.png)

### 10-2. Regressors

Electricity consumption is highly correlated to the number of household members of the respondent. When there are not too many household members, with more members, the quantity of consumption decreases, probably due to the fact that people tend to share the kitchen. However, the effect diminishes gradually when there are many household members (around 5-8).

```{r}
ti <- tibble(x = seq(1:8), y = 8.77946 + 0.03563 * seq(1:8)^2 - 
  0.52071 * seq(1:8) + 0.07329 * mean(dat$x8))

dat %>%
  mutate(index = row_number()) %>%
  ggplot() +
    geom_boxplot(aes(x2, y, group = cut_width(x2, 1)), outlier.alpha = 0) +
    geom_point(aes(x2, y), shape = 1) +
    geom_text(aes(x2, y, label = ifelse(index == 36 | index == 163 |
      index == 241, as.character(index), "")), hjust=1.5, vjust=0) +
    geom_line(data = ti, mapping = aes(x, y), color = "#F8766D", size = 2)
```

Because `y` is the log of average electricity consumptions, the final relationship between electricity consumptions and numbers of household members and numbers of rooms are:

As for the effect of number of rooms on electricity consumptions, 

```{r}
ti <- tibble(x = seq(2, 22), y = 8.77946 + 0.07329 * seq(2, 22) - 
  0.52071 * mean(dat$x2) + 0.03563 * mean(dat$x2^2))

dat %>%
  mutate(index = row_number()) %>%
  ggplot() +
    geom_boxplot(aes(x8, y, group = cut_width(x8, 1)), outlier.alpha = 0) +
    geom_point(aes(x8, y), shape = 1) +
    geom_line(data = ti, mapping = aes(x, y), color = "#F8766D", size = 2)
```

### 10-3. Orthogonalization of Multiple Regressors

```{r, echo=T}
mods[[8]] <-
  dat_2 %>%
  mutate(z1 = lm(x2 ~ 1)$residuals) %>%
  mutate(z2 = lm(x8 ~ x2 + 1)$residuals) %>%
  mutate(x2.2 = x2^2) %>%
  mutate(z3 = lm(x2.2 ~ x2 + x8 + 1)$residuals) %>%
  {lm(y ~ z1 + z2 + z3, data = .)}
```

```{r}
mods[[8]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

The intercept in `mods[[8]]` (8.37009) can be interpreted as the exptected value for an individual with average values of `x2`, `x8` and `I(x2^2)`, which can be verified by the prediction using `mods[[7]]`.

```{r, echo=T}
( 8.77946 - 0.52071 * mean(dat_2$x2) + 0.07329 * mean(dat_2$x8) + 
  0.03563 * mean(dat_2$x2^2) ) - 8.37009 <= 1e-4
```

Since the standard errors in `mods[[8]]` are smaller than those in `mods[[7]]`, `mods[[8]]` is used to conduct inference. Particularly, `se` for `Intercept` is reduced by 75.48%, and `se`s for first two regressors are reduced by 71.75% and 2.41%. The estimations for the last term are exactly the same, which is expected.

```{r}
mods[[8]] %>% confint()
```

To reduce average electricity consumptions, people are encouraged to live together in houses with fewer rooms.

<!-- ## 11. One-Sided t-Test of `mods[[1]]` -->

<!-- ```{r, echo=T} -->
<!-- mods[[1]] %>% -->
<!--   summary() %>% -->
<!--   {pt(coef(.)[2, 3], mods[[1]]$df, lower = FALSE)} %>% -->
<!--   {. <= qchisq(0.95, 1, lower.tail = TRUE, log.p = FALSE)} -->
<!-- ``` -->
