[
["LRA.html", "Chapter 2 Linear Regression Analysis", " Chapter 2 Linear Regression Analysis Following packages and functions are used in this project. ## basic packages library(knitr) library(kableExtra) library(tidyverse) library(conflicted) library(magrittr) library(broom) ## paticular packages for this project library(lmtest) library(corrr) library(tseries) library(corrplot) source(&quot;../src/funcs.R&quot;) source(&quot;../src/tests.R&quot;) The data set is defined as follows based on file recs.csv: set.seed(6) dat &lt;- read_csv(&quot;../data/recs.csv&quot;) %&gt;% dplyr::slice(sample(nrow(.), 300)) %&gt;% mutate(y = log(KWH / NHSLDMEM)) %&gt;% mutate(x8 = TOTROOMS + NCOMBATH + NHAFBATH) %&gt;% dplyr::select(y, x2 = NHSLDMEM, x3 = EDUCATION, x4 = MONEYPY, x5 = HHSEX, x6 = HHAGE, x7 = ATHOME, x8) %&gt;% mutate_at(seq(2, 8), as.integer) %&gt;% # make continuous variables discrete mutate(x5 = - x5 + 2) "],
["visualization.html", "2.1 Visualization", " 2.1 Visualization The first 5 rows of the data set used can be visualized: y x2 x3 x4 x5 x6 x7 x8 7.540 5 3 8 1 39 5 15 8.193 1 2 2 0 85 5 14 8.678 3 1 1 0 71 5 8 7.846 4 3 5 1 39 5 8 9.755 1 3 3 0 57 0 10 2.1.1 Covariance Matrix It can be seen that y is highly correlated to x2 and x6 according to the following table. x y r x2 y -0.51098 x3 y 0.03410 x4 y 0.04098 x5 y -0.04260 x6 y 0.35346 x7 y 0.03966 x8 y 0.21329 It can be seen from the following covariance matrix that y is highly correlated to x2, x6 and x8. Besides, x3-x4, x2-x6, x4-x8 are high correlated, which will be discussed in section 9. 2.1.2 Box Plot For each level of x2 a box indicating three quantiles (25%, 50%, 75%) of y is given. It shows that there is a tendency for y to decrease with x2 by looking at the median. The sizes of different boxes seem to vary with different values of x2. Besides, there are many observations when x2 is small. But it is assumed for now that the conditional variance is constant, which will be tested section 4. Three data points with extreme values 36, 241 and 163 is discussed in sections 3 and 5. The box plot of y by x6 is given. It can be seen that the tendency is not strictly linear and the condition variance is not stable. So we will regress y on x2 first and use x6 as the second regressor in section 6. "],
["mis-specification-analyis-msa.html", "2.2 Mis-Specification Analyis (MSA)", " 2.2 Mis-Specification Analyis (MSA) 2.2.1 Normality and Jarque-Bera Test of mods[[1]] The following four plots can be used to check the plausibility of normality assumptions: The upper left plot shows residuals against fitted values of mods[[1]]. It is hard to trust indication the flat trending line because there are few data points with low fitted values. The variance seems to be stable when fitted values are high. The assumption of homoskedasticity is tested formally in section 4. Data points 36, 241 and 163 are mentioned in all but the lower right plots. They are examined in section 6. The assumption of conditional normality looks reasonable according to the upper right Q-Q plot. A formal Jarque-Bera test is performed later this section to examine this assumption in a quantitative manner. The assumption of conditional normality is justified by JB test. whi stat df1 df2 p_value prob if_reject Jarque-Bera 4.326 2 298 0.115 0.05 FALSE 2.2.2 Homoskedasticity and White’s Test of mods[[1]] mods[[1]] cannot pass the White’s test, which means the variances of residuals do vary with different values of y. mods[[1]] %&gt;% test_white(dat, resi2 ~ x2 + I(x2^2), 2) %&gt;% tab_ti(F) whi stat df1 df2 p_value prob if_reject White 6.028 2 298 0.04909 0.05 TRUE 2.2.3 Functional Form and RESET Test of mods[[1]] mods[[1]] can pass RESET test. #&gt; Registered S3 methods overwritten by &#39;lme4&#39;: #&gt; method from #&gt; cooks.distance.influence.merMod car #&gt; influence.merMod car #&gt; dfbeta.influence.merMod car #&gt; dfbetas.influence.merMod car whi stat df1 df2 p_value prob if_reject RESET 1.128 1 299 0.2883 0.05 FALSE "],
["orthogonalization.html", "2.3 Orthogonalization", " 2.3 Orthogonalization 2.3.1 Regress y on x2, Assumptions and Orthogonalization mods[[1]] is obtained by regressing y on x2. #&gt; lm(formula = y ~ x2, data = dat) term estimate std.error statistic p.value (Intercept) 9.036 0.07526 120.06 2.214e-254 x2 -0.270 0.02631 -10.26 2.352e-21 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual 0.2611 0.2586 0.6309 105.3 2.352e-21 2 -286.5 579 590.1 118.6 298 By orthogonalizing x2 with respect to constant 1. the following reparameterized model can be obtained. mods[[2]] &lt;- dat %&gt;% mutate(x1 = 1, x21 = x2 - mean(.$x2)) %&gt;% dplyr::select(y, x1, x21) %&gt;% {lm(y ~ x1 + x21, data = .)} #&gt; lm(formula = y ~ x1 + x21, data = .) term estimate std.error statistic p.value (Intercept) 8.36 0.03642 229.52 0.000e+00 x21 -0.27 0.02631 -10.26 2.352e-21 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual 0.2611 0.2586 0.6309 105.3 2.352e-21 2 -286.5 579 590.1 118.6 298 The estimated regression coefficient for x2 in mods[[1]] equals that for x21 in mods[[2]]. That is, slopes in these two models are the same. The standard error of the intercept is reduced by 51.60 %. "]
]
