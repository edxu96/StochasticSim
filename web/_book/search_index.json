[
["LRA.html", "Chapter 1 Linear Regression Analysis (LRA)", " Chapter 1 Linear Regression Analysis (LRA) Following packages and functions are used in this chapter: ## basic packages library(knitr) library(kableExtra) library(tidyverse) library(conflicted) library(magrittr) library(broom) ## paticular packages for this project library(lmtest) library(corrr) library(tseries) library(corrplot) library(car) library(perturb) source(&quot;../src/funcs.R&quot;) source(&quot;../src/tests.R&quot;) The data set is defined as follows based on file recs.csv: set.seed(6) dat_recs &lt;- read_csv(&quot;../data/recs.csv&quot;) %&gt;% dplyr::slice(sample(nrow(.), 300)) %&gt;% mutate(y = log(KWH / NHSLDMEM)) %&gt;% mutate(x8 = TOTROOMS + NCOMBATH + NHAFBATH) %&gt;% dplyr::select(y, x2 = NHSLDMEM, x3 = EDUCATION, x4 = MONEYPY, x5 = HHSEX, x6 = HHAGE, x7 = ATHOME, x8) %&gt;% mutate_at(seq(2, 8), as.integer) %&gt;% # make continuous variables discrete mutate(x5 = - x5 + 2) The dataset delivery is from (Montgomery, Peck, and Vining 2012): dat_delivery &lt;- readxl::read_xls(&quot;../data/delivery.xls&quot;, col_names = c(&quot;i&quot;, &quot;time&quot;, &quot;case&quot;, &quot;dist&quot;), skip = 1) The dataset acetylene is from (Montgomery, Peck, and Vining 2012): dat_acetylene &lt;- readxl::read_xls(&quot;../data/acetylene.xls&quot;, col_names = c(&quot;i&quot;, &quot;p&quot;, &quot;t_raw&quot;, &quot;h_raw&quot;, &quot;c_raw&quot;), skip = 1) %&gt;% mutate(t = (t_raw - 1212.5) / 80.623) %&gt;% mutate(h = (h_raw - 12.44) / 5.662) %&gt;% mutate(c = (c_raw - 0.0403) / 0.03164) %&gt;% select(i, p, t, h, c) Simulation Generator "],
["to-learn.html", "1.1 To-Learn", " 1.1 To-Learn Confidence Interval MSA Likelihood Ratio Test ANOVA Orthogonalization "],
["visualization.html", "1.2 Visualization", " 1.2 Visualization The first 5 rows of the dat_recsa set used can be visualized: y x2 x3 x4 x5 x6 x7 x8 7.540 5 3 8 1 39 5 15 8.193 1 2 2 0 85 5 14 8.678 3 1 1 0 71 5 8 7.846 4 3 5 1 39 5 8 9.755 1 3 3 0 57 0 10 1.2.1 Covariance Matrix x y r x2 y -0.51098 x3 y 0.03410 x4 y 0.04098 x5 y -0.04260 x6 y 0.35346 x7 y 0.03966 x8 y 0.21329 1.2.2 Box Plot For each level of x2 a box indicating three quantiles (25%, 50%, 75%) of y is given. It shows that there is a tendency for y to decrease with x2 by looking at the median. The sizes of different boxes seem to vary with different values of x2. Besides, there are many observations when x2 is small. But it is assumed for now that the conditional variance is constant, which will be tested section 4. Three dat_recsa points with extreme values 36, 241 and 163 is discussed in sections 3 and 5. The box plot of y by x6 is given. It can be seen that the tendency is not strictly linear and the condition variance is not stable. So we will regress y on x2 first and use x6 as the second regressor in section 6. "],
["multi.html", "1.3 Multiple Linear Regression", " 1.3 Multiple Linear Regression 1.3.1 Standardization 1.3.2 Multicollinearity Diagnosis In some situations the regressors are nearly perfectly linearly related, and in such cases the inferences based on the regression model can be misleading or erroneous. When there are near-linear dependencies among the regressors, the problem of multicollinearity is said to exist. (9, Montgomery, Peck, and Vining 2012) There are four primary sources of multicollinearity: (9, Montgomery, Peck, and Vining 2012) The data collection method employed Constraints on the model or in the population Model specification An overdefined model To really establish causation, it is usually necessary to do an experiment in which the putative causative variable is manipulated to see what effect it has on the response. (Wood 2017,:1.5.7) (1) Covariance Matrix Inspection of the covariance matrix is not sufficient for detecting anything more complex than pair- wise multicollinearity. (Montgomery, Peck, and Vining 2012, 821:9.4.1 Examination of the Correlation Matrix) Example 1.1 It can be seen from the following covariance matrix that y is highly correlated to x2, x6 and x8. Besides, x3-x4, x2-x6, x4-x8 are high correlated. Figure 1.1: Heat map for the covariance matrix of recs. (2) Variance Inflation Factors (VIF) The collinearity diagnostics in R require the packages “perturb” and “car”. The R code to generate the collinearity diagnostics for the delivery data is: #&gt; case dist #&gt; 3.118474 3.118474 #&gt; Condition #&gt; Index Variance Decomposition Proportions #&gt; intercept case dist #&gt; 1 1.000 0.041 0.015 0.016 #&gt; 2 3.240 0.959 0.069 0.076 #&gt; 3 6.378 0.000 0.915 0.909 #&gt; x2 x3 x4 x5 x6 x7 x8 #&gt; 1.360196 1.494892 1.853040 1.049184 1.383186 1.094072 1.559469 #&gt; Condition #&gt; Index Variance Decomposition Proportions #&gt; intercept x2 x3 x4 x5 x6 x7 x8 #&gt; 1 1.000 0.001 0.003 0.002 0.003 0.006 0.001 0.005 0.002 #&gt; 2 3.542 0.000 0.004 0.001 0.035 0.730 0.000 0.001 0.006 #&gt; 3 4.554 0.000 0.026 0.002 0.100 0.112 0.013 0.472 0.001 #&gt; 4 5.164 0.000 0.518 0.027 0.037 0.004 0.021 0.018 0.003 #&gt; 5 6.768 0.022 0.011 0.004 0.256 0.048 0.147 0.486 0.011 #&gt; 6 9.250 0.015 0.018 0.555 0.067 0.073 0.009 0.012 0.376 #&gt; 7 10.622 0.011 0.007 0.225 0.498 0.000 0.206 0.005 0.598 #&gt; 8 17.214 0.951 0.411 0.184 0.004 0.026 0.602 0.001 0.004 1.3.3 Orthogonalization Simulation Generator "],
["inference.html", "1.4 Inference", " 1.4 Inference 1.4.1 Confidence Interval By choosing a 95% coverage, we accept that with 5% confidence we reach the false conclusion that the true parameter is not in the confidence interval. (??? Confidence intervals) #&gt; 2.5 % 97.5 % #&gt; (Intercept) 8.156353087 8.918779651 #&gt; x2 -0.345668390 -0.232993887 #&gt; x3 -0.173798148 -0.029166657 #&gt; x4 -0.015521841 0.063752184 #&gt; x5 -0.140484745 0.133522728 #&gt; x6 -0.001040888 0.008468877 #&gt; x7 -0.028330570 0.039288527 #&gt; x8 0.041431470 0.095149773 1.4.2 Likelihood Ratio Test 1.4.3 Signed Likelihood Ratio Test "]
]
