---
title: "Linear Regression Analysis of Electricity Consumption"
output: 
  html_notebook: 
    code_folding: hide
    fig_caption: false
    fig_height: 7.5
    fig_width: 15
    theme: default
    toc: true
    toc_float: true
    smooth_scroll: true
    df_print: paged
author: Edward J. Xu (<edxu96@outlook.com>)
date: "`r Sys.Date()`"
editor_options:
  chunk_output_type: console
---

## Latest Updates

- Comments for graphic check of the contant variance assumption in section 3 are modified. From the left two plots, the assumption seems to hold, so we need the formal White's test.
- The cross correlation matrix of regressors is visualized in section 1.
- The value 1 of `x5` will indicate a female respondent, while male respondents are represented using 0 instead 2. This modification will make it easier to intepret the intercepts in models containing `x5`.
- Correct the degree of freedom in White's test for `mods[[5]]`.
- Add hand-calculation RESET test and JB test.
- Update section 8 causality.
- Present the final model `mods[[7]]`.

## To-Do

- To learn about sign tests (one-sided test). See 14.2 in _ross2017introductory_.
- To discuss why we choose the final model.
- To intepret the final model using economics.
- Variance inflation factor

## Libraries and Data

```{r}
rm(list = ls())
```

Following packages and functions are used in the analysis.

```{r, message = FALSE, echo=T}
library(knitr)
library(kableExtra)
library(tidyverse)
library(conflicted)
library(magrittr)
library(lmtest)
library(corrr)
library(broom)
library(tseries)
library(car)
library(corrplot)
source("./funcs.R")
```

```{r, include=FALSE}
setwd("~/GitHub/TidySimStat/examples")
options(width=80)
knitr::opts_chunk$set(
  comment = "#>",
  echo = FALSE,
  fig.align="center"
)
```

The data set is defined as follows:

```{r, message = FALSE, echo=T}
set.seed(6)
dat <- 
  read_csv("./data/recs.csv") %>%
  dplyr::slice(sample(nrow(.), 300)) %>%
  mutate(y = log(KWH / NHSLDMEM)) %>%
  mutate(x8 = TOTROOMS + NCOMBATH + NHAFBATH) %>%
  dplyr::select(y, x2 = NHSLDMEM, x3 = EDUCATION, x4 = MONEYPY, x5 = HHSEX, 
    x6 = HHAGE, x7 = ATHOME, x8) %>%
  mutate_at(seq(2, 8), as.integer) %>%  # make continuous variables discrete
  mutate(x5 = - x5 + 2) 
```

Different variables are summarized in the following table.

| Sym | Abbr       | Definition                              |
| --- | ---------- | --------------------------------------- |
| z   | KWH        | electricity consumption                 |
| y   | LKWH.pers  | logarithm of KWH/NHSLDMEM               |
| x2  | NHSLDMEM   | number of household members             |
| x3  | EDUCATION  | highest education completed             |
| x4  | MONEYPY    | annual gross household income last year |
| x5  | HHSEX      | gender                                  |
| x6  | HHAGE      | age                                     |
| x7  | ATHOME     | number of weekdays someone is at home   |
| x8  | TOTROOMS + | number of rooms (including bathrooms)   |

The first 5 rows can be visualized:

```{r}
head(dat, 5) %>%
  kable() %>%
  kable_styling(bootstrap_options = "condensed", full_width = F)
```

## 1. Data Visualization

It can be seen that `y` is highly correlated to `x2` and `x6` according to the following table.

```{r}
dat %>%
  cor() %>%
  as_cordf() %>%
  stretch() %>%
  dplyr::filter(y == "y" & x != "y") %>%
  kable() %>%
  kable_styling(bootstrap_options = "condensed", full_width = F)
```

It can be seen from the following covariance matrix that `y` is highly correlated to `x2`, `x6` and `x8`. Besides, `x3`-`x4`, `x2`-`x6`, `x4`-`x8` are high correlated, which will be discussed in section 9.

```{r}
dat %>%
  cor() %>%
  corrplot(type = "upper", tl.col = "black", tl.srt = 45)
```

For each level of `x2` a box indicating three quantiles (25%, 50%, 75%) of `y` is given. It shows that there is a tendency for `y` to decrease with `x2` by looking at the median. The sizes of different boxes seem to vary with different values of `x2`. Besides, there are many observations when `x2` is small. But it is assumed for now that the conditional variance is constant, which will be tested section 4. Three data points with extreme values `36`, `241` and `163` is discussed in sections 3 and 5. 

```{r}
dat %>%
  mutate(index = row_number()) %>%
  ggplot() +
    geom_boxplot(aes(x2, y, group = cut_width(x2, 1)), outlier.alpha = 0) +
    geom_point(aes(x2, y), shape = 1) +
    geom_text(aes(x2, y, label = ifelse(index == 36 | index == 163 |
      index == 241, as.character(index), "")), hjust=1.5, vjust=0)
```

The box plot of `y` by `x6` is given. It can be seen that the tendency is not strictly linear and the condition variance is not stable. So we will regress `y` on `x2` first and use `x6` as the second regressor in section 6.

```{r}
dat %>%
  ggplot() +
  geom_boxplot(aes(x6, y, group = cut_width(x6, 10)), outlier.alpha = 0)+
    geom_point(aes(x6, y), shape = 1)
```

## 2. Regress `y` on `x2`

`mods[[1]]` is obtained by regressing `y` on `x2`.

```{r}
mods <- list()
mods[[1]] <- lm(y ~ x2, data = dat)
mods[[1]] %>% summary()

results <- new_results(mods[[1]], 1)
```

By orthogonalizing `x2` with respect to constant 1. the following reparameterized model can be obtained.

```{r, echo = T}
mods[[2]] <- 
  dat %>%
  mutate(x1 = 1, x21 = x2 - mean(.$x2)) %>%
  dplyr::select(y, x1, x21) %>%
  {lm(y ~ x1 + x21, data = .)}

results %<>%
  collect_glance(mods[[2]], 2)

mods[[2]] %>% 
  tidy()%>%
  kable() %>%
  kable_styling(bootstrap_options = "condensed", full_width = F)
```

The estimated regression coefficient for `x2` in `mods[[1]]` equals that for `x21` in `mods[[2]]`. That is, slopes in these two models are the same.

## 3. Normality and Jarque-Bera Test of `mods[[1]]`

The following four plots can be used to check the plausibility of normality assumptions:

- The upper left plot shows residuals against fitted values of `mods[[1]]`. It is hard to trust indication the flat trending line because there are few data points with low fitted values. The variance seems to be stable when fitted values are high. The assumption of homoskedasticity is tested formally in section 4.
- Data points 36, 241 and 163 are mentioned several times. They are examined in section 5.
- The assumption of conditional normality looks reasonable according to the upper right Q-Q plot. A formal Jarque-Bera test is performed later this section to examine this assumption in a quantitative manner.

```{r, warning=FALSE}
par_orginal <- par()
par(mfrow = c(2, 2), mai = c(0.3, 0.3, 0.3, 0.3))
plot(mods[[1]])
par(par_orginal)
```

The assumption of conditional normality is not justified by JB test.

```{r, echo = T}
mods[[1]] %>%
  test_jb(dat)
```

Conditional distributions are visualized:

```{r}
dat %>%
  dplyr::filter(x2 == 1 | x2 == 3 | x2 == 5 | x2 == 7 | x2 == 9) %>%
  ggplot() + 
    geom_freqpoly(aes(y)) +
    facet_grid(rows = vars(x2))
```

## 4. Homoskedasticity and White's Test of `mods[[1]]`

`mods[[1]]` cannot pass the White's test, which means the variances of residuals do vary with different values of `y`.

```{r, echo = T}
mods[[1]] %>%
  test_white(dat, resi2 ~ x2 + I(x2^2), 2)
```

## 5. Functional Form and RESET Test of `mods[[1]]`

`mods[[1]]` can pass RESET test.

```{r}
mods[[1]] %>%
  test_reset(dat)
```

## 6. Regress `y` on `x2` with `36` Data Points Excluded

According to the scatter plot in section 1, the values of `y` in data points `36`, `241` and `163` are too small. With just 8 other points when `x2` equals 6, data point `36` will have a huge impact on the model, so it is excluded in the following model.

```{r}
dat %>%
  mutate(index = row_number()) %>%
  dplyr::filter(row_number() == 241 | row_number() == 163 | 
    row_number() == 36) %>%
  dplyr::select(index, y, x2) %>%
  kable() %>%
  kable_styling(full_width = F)
```

```{r}
mods[[2]] <-
  dat %>%
  dplyr::filter(row_number() != 36) %>%
  {lm(y ~ x2, data = .)}

mods[[2]] %>% summary()
```

Compared with `mods[[1]]`, `mods[[2]]` has more accurate estimation. Besides, `mods[[2]]` passes RESET test and White's test at the same time. So data point `36` is exluded in the following models, and the corresponding new data set `dat_2` is used.

```{r}
dat_2 <-
  dat %>%
  dplyr::filter(row_number() != 36)

results_test <-
  mods[[2]] %>%
  test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[2]], dat_2, resi2 ~ x2 + I(x2^2), 2)) %>%
  bind_rows(test_reset(mods[[2]], dat_2))

results_test %>%
  kable() %>%
  kable_styling(bootstrap_options = "condensed", full_width = F)
```

## 7. Models with More Regressors

`mods[[3]]` with `x2` and `x6` being regressors shows:

```{r}
mods[[3]] <- lm(y ~ x2 + x6, data = dat_2)
results %<>% collect_glance(mods[[3]], 3)

mods[[3]] %>%
  tidy() %>%
  kable() %>%
  kable_styling(bootstrap_options = "condensed", full_width = F)
```

`mods[[4]]` with all variables being regressors shows:

```{r}
mods[[4]] <- lm(y ~ x2 + x3 + x4 + x5 + x6 + x7, data = dat_2)
results %<>% collect_glance(mods[[4]], 4)

mods[[4]] %>%
  tidy() %>%
  kable() %>%
  kable_styling(bootstrap_options = "condensed", full_width = F)
```

`mods[[5]]` is determined by automated model selection using `mods[[4]]` with `stats::step` function.

```{r}
mods[[5]] <- step(mods[[4]])
results %<>%
  collect_glance(mods[[5]], 5)
```

According to results of five models, `mods[[5]]` with expression `y ~ x2 + x3 + x4 + x6` is choosen for now. Mis-specification analysis is performed in the next section 7.

```{r}
results %>%
  kable() %>%
  kable_styling(bootstrap_options = "condensed", full_width = F)
```

`mods[[5]]` can pass two White's tests, but cannot pass Jarque-Bera test and RESET test.

```{r, echo = T}
results_test <-
  mods[[5]] %>%
  test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[5]], dat_2, resi2 ~ x2 + x3 + x4 + x6 + I(x2^2) +
    I(x3^2) + I(x4^2) + I(x6^2), 5)) %>%
  bind_rows(test_white(mods[[5]], dat_2, resi2 ~ x2 + x3 + x4 + x6 + 
    I(x2^2) + I(x3^2) + I(x4^2) + I(x6^2) + I(x2 * x3) + I(x2 * x4) + 
    I(x2 * x6) + I(x4 * x3) + I(x6 * x3) +I(x4 * x6), 15)) %>%
  bind_rows(test_reset(mods[[5]], dat_2))

results_test %>%
  kable() %>%
  kable_styling(bootstrap_options = "condensed", full_width = F)
```

`mods[[4]]` can pass two White's tests, but cannot pass Jarque-Bera test and RESET test.

```{r, echo = T}
results_test <-
  mods[[4]] %>%
  test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[4]], dat_2, resi2 ~ x2 + x3 + x4 + x5 + x6 + x7 + 
    I(x2^2) + I(x3^2) + I(x4^2) + I(x5^2) + I(x6^2) + I(x7^2), 7)) %>%
  bind_rows(test_white(mods[[4]], dat_2, resi2 ~ x2 + x3 + x4 + x5 + x6 + x7 + 
    I(x2^2) + I(x3^2) + I(x4^2) + I(x5^2) + I(x6^2) + I(x7^2) + I(x2 * x3) +
    I(x2 * x4) + I(x2 * x5) + I(x2 * x6) + I(x2 * x7) + I(x4 * x3) + 
    I(x5 * x3) + I(x6 * x3) + I(x3 * x7) + I(x4 * x5) + I(x4 * x6) + 
    I(x4 * x7) + I(x5 * x6) + I(x5 * x7) + I(x6 * x7), 28)) %>%
  bind_rows(test_reset(mods[[4]], dat_2))

results_test %>%
  kable() %>%
  kable_styling(bootstrap_options = "condensed", full_width = F)
```

`mods[[3]]` can pass all tests.

```{r, echo = T}
results_test <-
  mods[[3]] %>%
  test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[3]], dat_2, resi2 ~ x2 + x6 + I(x2^2) + I(x6^2),
    3)) %>%
  bind_rows(test_white(mods[[3]], dat_2, resi2 ~ x2 + x6 + I(x2^2) + I(x6^2) +
    I(x2 * x6), 6)) %>%
  bind_rows(test_reset(mods[[3]], dat_2))

results_test %>%
  kable() %>%
  kable_styling(bootstrap_options = "condensed", full_width = F)
```

## 8. Causality

### 8-1. Causality of `x2`-`x6`

It can be seen from the following two models that `y` is highly correlated to `x2` and `x6` separately. Besides, according to `mods[[3]]`, `y` is highly correlated to `x2` and `x6` at the same time. 

```{r, echo = T}
mods_62 <- list()
mods_62[[1]] <- lm(y ~ x2, data = dat_2)
mods_62[[1]] %>% summary()
```

```{r, echo = T}
mods_62[[2]] <- lm(y ~ x6, data = dat_2)
mods_62[[2]] %>% summary()
```

It is reasonable to assume that people tend to have more accompanies as age increases after 18. Also, the following model proves the relationship between `x2` and `x6`. We can say that the effect of `x6` on `y` is mediated by `x2`. With `x6` affecting `y` as well, `x2` does not mediate `x6` completely. So `x2` and `x6` are both supposed to appear in the model for `y`. The mediation factor `x2` may affect `y` though other ways, which will be explored in section 10. Besides, we find that `x6` does not affect `y` in other ways in section 10.

```{r, echo = T}
mods_62[[3]] <- lm(x2 ~ x6, data = dat_2)
mods_62[[3]] %>% summary()
```

### 8-2. Causality of `x3`-`x4`

When taking `x3` and `x4` into consideration, the models assocating `y` with `x3` or `x4` both show no significance, though `x3` and `x4` are highly correlated. So neither `x3` nor `x4` should be included in the model.

### 8-3. Causality of `x4`-`x8`

It can be seen that `y` is highly correlated to `x8`.

```{r}
mods_48 <- list()
mods_48[[1]] <-  lm(y ~ x8, data = dat_2)
mods_48[[1]] %>% summary()
```

Also, `x8` is highly related to `x4`, which makes sense, because people with more income tend to buy houses with more rooms.

```{r}
mods_48[[2]] <-  lm(x8 ~ x4, data = dat_2)
mods_48[[2]] %>% summary()
```

However, from subsection 9.2 we already know that `y` is not highly correlated with `x4`, which is illustrated again by the following model. So we say that the effect of `x4` on `y` is completely mediated by `x8`. Though it makes sense that people with more income tend to consume more energy on average, the direct effect is mediated through `x8` and possibly other factors.

```{r}
mods_48[[3]] <-  lm(y ~ x8 + x4, data = dat_2)
mods_48[[3]] %>% summary()
```

## 9. `mods[[3]]` with extra terms

In `mods[[6]]`, `x2`, `x6` and `x8` are kept at the same time.

```{r, echo = T}
mods[[6]] <- lm(y ~ x2 + x6 + x8, data = dat_2)
mods[[6]] %>% summary()
```

```{r}
results_test <-
  mods[[6]] %>%
  test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[6]], dat_2, resi2 ~ x2 + x6 + x8 + I(x2^2) +
    I(x6^2) + I(x8^2), 4)) %>%
  bind_rows(test_white(mods[[6]], dat_2, resi2 ~ x2 + x6 + x8 + I(x2^2) +
    I(x6^2) + I(x8^2) + I(x2 * x6) + I(x2 * x8) + I(x6 * x8), 10)) %>%
  bind_rows(test_reset(mods[[6]], dat_2))

results_test %>%
  kable() %>%
  kable_styling(bootstrap_options = "condensed", full_width = F)
```

In `mods[[7]]`, `x6` is excluded because a new term `x2^2` is added, and `x6` becomes insignificant. The effect of `x6` on `y` seems to be mediated compeletely by `x2` and `x2^2`.

```{r}
mods[[7]] <- lm(y ~ x2 + x8 + I(x2^2), data = dat_2)
mods[[7]] %>% summary()
```

```{r}
results_test <-
  mods[[7]] %>%
  test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[7]], dat_2, resi2 ~ x2 + x8 + I(x2^2) + I(x8^2),
    3)) %>%
  bind_rows(test_white(mods[[7]], dat_2, resi2 ~ x2 + x8 + I(x2^2) +
    I(x8^2) + I(x2 * x8), 6)) %>%
  bind_rows(test_reset(mods[[7]], dat_2))

results_test %>%
  kable() %>%
  kable_styling(bootstrap_options = "condensed", full_width = F)
```

`mods[[7]]` has a lower AIC than `mods[[6]]` and `mods[[3]]`.

```{r}
AIC(mods[[7]], mods[[6]], mods[[3]]) %>%
  kable() %>%
  kable_styling(bootstrap_options = "condensed", full_width = F)
```

## 10. One-Sided t-Test of `mods[[1]]`

```{r, echo = T}
mods[[1]] %>%
  summary() %>%
  {pt(coef(.)[2, 3], mods[[1]]$df, lower = FALSE)} %>%
  {. <= qchisq(0.95, 1, lower.tail = TRUE, log.p = FALSE)}
```