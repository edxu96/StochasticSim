---
title: "Linear Regression Analysis of Electricity Consumption"
author: "Edward J. Xu (<edxu96@outlook.com>)"
date: "`r Sys.Date()`"
documentclass: article
classoption: a4paper
output:
  html_notebook:
    code_folding: hide
    df_print: paged
    fig_caption: no
    fig_height: 7.5
    fig_width: 15
    smooth_scroll: yes
    theme: default
    toc: yes
    toc_float: no
  #   toc: yes
  #   keep_tex: yes
  # pdf_document:
editor_options:
  chunk_output_type: inline
---

## Introduction

```{r}
rm(list = ls())
```

Following packages and functions are used in this project.

```{r, message = FALSE, echo=T}
## basic packages
library(knitr)
library(kableExtra)
library(tidyverse)
library(conflicted)
library(magrittr)
library(broom)
# library(car)
## paticular packages for this project
library(lmtest)
library(corrr)
library(tseries)
library(corrplot)
source("./funcs.R")
```

```{r, include=FALSE}
setwd("~/GitHub/TidySimStat/examples/regress")
options(width = 80, pillar.sigfig = 5)
knitr::opts_chunk$set(
  comment = "#>",
  echo = FALSE,
  fig.align="center"
)
```

The data set is defined as follows based on file `recs.csv`:

```{r, message = FALSE, echo=T}
set.seed(6)
dat <-
  read_csv("../../data/recs.csv") %>%
  dplyr::slice(sample(nrow(.), 300)) %>%
  mutate(y = log(KWH / NHSLDMEM)) %>%
  mutate(x8 = TOTROOMS + NCOMBATH + NHAFBATH) %>%
  dplyr::select(y, x2 = NHSLDMEM, x3 = EDUCATION, x4 = MONEYPY, x5 = HHSEX,
    x6 = HHAGE, x7 = ATHOME, x8) %>%
  mutate_at(seq(2, 8), as.integer) %>%  # make continuous variables discrete
  mutate(x5 = - x5 + 2)
```

Different variables are summarized in the following table. `y`, the logarithm of averaged electricity consumption, is the variable that we are interested in. Specifically, The electricity consumption refers to the electricity usage of the house/studio where the respondent lives in 2015, measured in kilowatthours. The quantity is average by the number of household members in the house/studio. That way, it roughly represent the level of electricity consumption of the respondent. Other variables are discussion in the following table.

| Sym | Abbr       | Definition                              |
| --- | ---------- | --------------------------------------- |
| z   | KWH        | electricity consumption                 |
| y   | LKWH.pers  | logarithm of KWH/NHSLDMEM               |
| x2  | NHSLDMEM   | number of household members             |
| x3  | EDUCATION  | highest education completed             |
| x4  | MONEYPY    | annual gross household income last year |
| x5  | HHSEX      | gender                                  |
| x6  | HHAGE      | age                                     |
| x7  | ATHOME     | number of weekdays someone is at home   |
| x8  | TOTROOMS + | number of rooms (including bathrooms)   |

Note that `x8` is a variable indicating the number of rooms of the house/studio of the respondent. It equals the summation of `TOTROOMS`, `NCOMBATH` and `NHAFBATH` in the original data set. `x8` is not included in the initial analysis (sections 1 to 7). 

`x3`, the education level of the respondent, is considered as a continuous variable in this project for simplicity. The detailed definition of different levels is shown in the following table.

| Level | Definition                                  |
| ----- | ------------------------------------------- |
| 1     | Less than high school diploma or GED        |
| 2     | High school diploma or GED                  |
| 3     | Some college or Associate’s degree          |
| 4     | Bachelor’s degree (for example: BA, BS)     |
| 5     | Master’s, Professional, or Doctorate degree |

The first 5 rows of the data set used can be visualized:

```{r}
head(dat, 5) %>% tab_ti(F)
```

In this project, we want to develop a model associating the continuous variable, average electricity consumption, with other variables. We will start by visualizing correlations between variables. In particular, the two variables `x2` and `x6`, which highly correlate with `y` will be explored. The model regressing `y` on `x2` will be discussed in section 2. Four assumptions will be made, three of which will be tested in section 3-5 by Jarque-Bera test (normality), White’s test (homoskedasticity) and RESET test (functional form) respectively. According the test results and discussion in section 6, data point `36` is excluded. Then, in section 7, more regressors are introduced and three new models are established. The detail regarding how regressors interact with each other, namely causality, is discussed in section 8. Based on the understanding of the underlying mechanism, a nonlinear term and `x8` are included as new regressors. Two new models are estimated, based on the only model passing all tests in section 7. Finally, the model called `mods[[7]]` is chosen as the final for presentation and it is interpreted in section 10.

## 1. Data Visualization

It can be seen that `y` is highly correlated to `x2` and `x6` according to the following table.

```{r}
dat %>%
  cor() %>%
  as_cordf() %>%
  stretch() %>%
  dplyr::filter(y == "y" & x != "y") %>%
  tab_ti(F)
```

It can be seen from the following covariance matrix that `y` is highly correlated to `x2`, `x6` and `x8`. Besides, `x3`-`x4`, `x2`-`x6`, `x4`-`x8` are high correlated, which will be discussed in section 9.

```{r}
dat %>%
  cor() %>%
  corrplot(type = "upper", tl.col = "black", tl.srt = 45)
```

For each level of `x2` a box indicating three quantiles (25%, 50%, 75%) of `y` is given. It shows that there is a tendency for `y` to decrease with `x2` by looking at the median. The sizes of different boxes seem to vary with different values of `x2`. Besides, there are many observations when `x2` is small. But it is assumed for now that the conditional variance is constant, which will be tested section 4. Three data points with extreme values `36`, `241` and `163` is discussed in sections 3 and 5. 

```{r}
dat %>%
  mutate(index = row_number()) %>%
  ggplot() +
    geom_boxplot(aes(x2, y, group = cut_width(x2, 1)), outlier.alpha = 0) +
    geom_point(aes(x2, y), shape = 1) +
    geom_text(aes(x2, y, label = ifelse(index == 36 | index == 163 |
      index == 241, as.character(index), "")), hjust=1.5, vjust=0)
```

The box plot of `y` by `x6` is given. It can be seen that the tendency is not strictly linear and the condition variance is not stable. So we will regress `y` on `x2` first and use `x6` as the second regressor in section 6.

```{r}
dat %>%
  ggplot() +
  geom_boxplot(aes(x6, y, group = cut_width(x6, 10)), outlier.alpha = 0)+
    geom_point(aes(x6, y), shape = 1)
```

## 2. Regress `y` on `x2`, Assumptions and Orthogonalization

`mods[[1]]` is obtained by regressing `y` on `x2`.

```{r}
mods <- list()
mods[[1]] <- lm(y ~ x2, data = dat)

mods[[1]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)

results <- new_results(mods[[1]], 1)
```

By orthogonalizing `x2` with respect to constant 1. the following reparameterized model can be obtained.

```{r, echo=T}
mods[[2]] <- 
  dat %>%
  mutate(x1 = 1, x21 = x2 - mean(.$x2)) %>%
  dplyr::select(y, x1, x21) %>%
  {lm(y ~ x1 + x21, data = .)}
```

```{r}
results %<>%
  collect_glance(mods[[2]], 2)

mods[[2]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

The estimated regression coefficient for `x2` in `mods[[1]]` equals that for `x21` in `mods[[2]]`. That is, slopes in these two models are the same. The standard error of the intercept is reduced by 51.60 %.

## 3. Normality and Jarque-Bera Test of `mods[[1]]`

The following four plots can be used to check the plausibility of normality assumptions:

- The upper left plot shows residuals against fitted values of `mods[[1]]`. It is hard to trust indication the flat trending line because there are few data points with low fitted values. The variance seems to be stable when fitted values are high. The assumption of homoskedasticity is tested formally in section 4.
- Data points `36`, `241` and `163` are mentioned in all but the lower right plots. They are examined in section 6.
- The assumption of conditional normality looks reasonable according to the upper right Q-Q plot. A formal Jarque-Bera test is performed later this section to examine this assumption in a quantitative manner.

```{r, warning=FALSE}
par_orginal <- par()
par(mfrow = c(2, 2), mai = c(0.3, 0.3, 0.3, 0.3))
plot(mods[[1]])
par(par_orginal)
```

The assumption of conditional normality is justified by JB test.

```{r}
mods[[1]] %>% test_jb(dat) %>% tab_ti(F)
```

## 4. Homoskedasticity and White's Test of `mods[[1]]`

`mods[[1]]` cannot pass the White's test, which means the variances of residuals do vary with different values of `y`.

```{r, echo=T}
mods[[1]] %>% test_white(dat, resi2 ~ x2 + I(x2^2), 2) %>% tab_ti(F)
```

## 5. Functional Form and RESET Test of `mods[[1]]`

`mods[[1]]` can pass RESET test.

```{r}
mods[[1]] %>% test_reset(dat) %>% tab_ti(F)
```

## 6. Regress `y` on `x2` with `36` Data Point Excluded

`36`, `241` and `163` data points are mentioned in three plots regrading the analysis of residuals of `mods[[1]]` in section 3. According to the scatter plot in section 1, their values of `y` are too small compared to those with same values of `x2`. They seems to be well defined. 

```{r}
dat %>%
  mutate(index = row_number()) %>%
  dplyr::filter(row_number() == 241 | row_number() == 163 | 
    row_number() == 36) %>%
  dplyr::select(index, y, x2) %>%
  tab_ti(F)
```

However, with just 8 other points when `x2` equals 6, data point `36` will have a huge impact on the model, so it is excluded in the following model `mods[[2]]`. That is, a new model is estimated with the same formula as `mods[[1]]` but the data set excluding data point `36`.

```{r}
mods[[2]] <-
  dat %>%
  dplyr::filter(row_number() != 36) %>%
  {lm(y ~ x2, data = .)}

mods[[2]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

Compared with `mods[[1]]`, `mods[[2]]` has more accurate estimation. Besides, `mods[[2]]` passes all of the hypothesis tests. So data point `36` is exluded in the following models, and the corresponding new data set `dat_2` is used.

```{r}
dat_2 <-
  dat %>%
  dplyr::filter(row_number() != 36)

results_test <-
  mods[[2]] %>%
  test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[2]], dat_2, resi2 ~ x2 + I(x2^2), 2)) %>%
  bind_rows(test_reset(mods[[2]], dat_2))

results_test %>% tab_ti(F)
```

## 7. Models with More Regressors

### 7-1. Benchmark Model

`mods[[3]]` with `x2` and `x6` being regressors is a good model already and pass every test. It is chosen as the benchmark model after the discussion in subsection 7-4.

```{r}
mods[[3]] <- lm(y ~ x2 + x6, data = dat_2)
results %<>% collect_glance(mods[[3]], 3)

## Print model summary
model <- mods[[3]]
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

```{r, echo=T}
results_test <- mods[[3]] %>% test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[3]], dat_2, resi2 ~ x2 + x6 + I(x2^2) + I(x6^2),
    3)) %>%
  bind_rows(test_white(mods[[3]], dat_2, resi2 ~ x2 + x6 + I(x2^2) + I(x6^2) +
    I(x2 * x6), 6)) %>%
  bind_rows(test_reset(mods[[3]], dat_2))

results_test %>% tab_ti(F)
```

### 7-2. Model with All Regressors

To construct `mods[[4]]`, all of the variables (excluding `x8`) are included. `mods[[4]]` can pass Jarque-Bera test and two White's tests, but cannot pass  RESET test.

```{r}
mods[[4]] <- lm(y ~ x2 + x3 + x4 + x5 + x6 + x7, data = dat_2)
results %<>% collect_glance(mods[[4]], 4)
mods[[4]] %>% tab_tidy(T)
```

```{r, echo=T}
results_test <- mods[[4]] %>% test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[4]], dat_2, resi2 ~ x2 + x3 + x4 + x5 + x6 + x7 + 
    I(x2^2) + I(x3^2) + I(x4^2) + I(x5^2) + I(x6^2) + I(x7^2), 7)) %>%
  bind_rows(test_white(mods[[4]], dat_2, resi2 ~ x2 + x3 + x4 + x5 + x6 + x7 + 
    I(x2^2) + I(x3^2) + I(x4^2) + I(x5^2) + I(x6^2) + I(x7^2) + I(x2 * x3) +
    I(x2 * x4) + I(x2 * x5) + I(x2 * x6) + I(x2 * x7) + I(x4 * x3) + 
    I(x5 * x3) + I(x6 * x3) + I(x3 * x7) + I(x4 * x5) + I(x4 * x6) + 
    I(x4 * x7) + I(x5 * x6) + I(x5 * x7) + I(x6 * x7), 28)) %>%
  bind_rows(test_reset(mods[[4]], dat_2))

results_test %>% tab_ti(F)
```

### 7-3. Likelihood Ratio Test

Likelihood ratio tests for restricting one parameter can be performed by using partial correlations. For example, to test the hypothesis that coefficient for `x5` is 0 in `mods[[4]]`, following calculation can be conducted. With `p_value` being 0.7581638, we cannot reject the hypothesis.

```{r, echo=T}
lr_x5 <- - 299 * log(1 - 0.0003170) 
(1 - pchisq(lr_x5, 1))
```

```{r}
mods_lik <- list()
mods_lik[[1]] <- lm(y ~ x2 + x3 + x4 + x6 + x7, data = dat_2)
test_lik(mods[[4]], mods_lik[[1]]) %>% tab_ti(F)
```

Likelihood tests for restricting more than one parameter can be only performed by using values of log likelihood in the original and restricted models. For example, to test the hypothesis that coefficients for `x5` and `x7` are both 0 in `mods[[4]]`, following calculation can be conducted. We cannot reject the hypothesis according the function output.

```{r}
mods_lik[[2]] <- lm(y ~ x2 + x3 + x4 + x6, data = dat_2)
test_lik(mods[[4]], mods_lik[[2]]) %>% tab_ti(F)
```

```{r}
test_lik(mods_lik[[1]], mods_lik[[2]]) %>% tab_ti(F)
```

The above three test statistics are related in an additive manner, so models with multiple regressors can be reduced in a step-wise procedure. During every step, partial correlations for regressors can be used as the indication of the next term to be reduced.

```{r}
test_lik(mods[[4]], mods_lik[[1]])$stat + 
  test_lik(mods_lik[[1]], mods_lik[[2]])$stat -
  test_lik(mods[[4]], mods_lik[[2]])$stat <= 1e-5
```

### 7-4. Automated Model Selection

Thus, `mods[[5]]` is determined by automated model selection using `mods[[4]]` with `stats::step` function. `mods[[5]]` can pass Jarque-Bera test and two White's tests, but cannot pass RESET test.

```{r}
mods[[5]] <- quiet(step(mods[[4]]))
results %<>% collect_glance(mods[[5]], 5)

mods[[5]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

```{r, echo=T}
results_test <- mods[[5]] %>% test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[5]], dat_2, resi2 ~ x2 + x3 + x4 + x6 + I(x2^2) +
    I(x3^2) + I(x4^2) + I(x6^2), 5)) %>%
  bind_rows(test_white(mods[[5]], dat_2, resi2 ~ x2 + x3 + x4 + x6 + 
    I(x2^2) + I(x3^2) + I(x4^2) + I(x6^2) + I(x2 * x3) + I(x2 * x4) + 
    I(x2 * x6) + I(x4 * x3) + I(x6 * x3) +I(x4 * x6), 15)) %>%
  bind_rows(test_reset(mods[[5]], dat_2))

results_test %>% tab_ti(F)
```

According to results of five models, though `mods[[4]]` and `mods[[5]]` have lower AIC, `mods[[3]]` is the one with all tests passed and lowst AIC. It is discussed intensively in subsection 8-1, and acts as the benchmark model in section 9. Besides, `mods[[3]]` has the lowest BIC.

```{r}
results %>% tab_ti(F)
```

Additionally, `mods[[4]]` has the highest `r.squared` for including more regressors, but its `adj.r.squared` is penalized for that. The AIC and BIC are not low as well compared to those of `mods[[3]]` and `mods[[5]]`. 

## 8. Causality and Mediation

### 8-1. Causality of `x2`-`x6`

It can be seen from the following two models that `y` is highly correlated to `x2` and `x6` separately. Besides, according to `mods[[3]]`, `y` is highly correlated to `x2` and `x6` at the same time. 

```{r}
mods_62 <- list()
mods_62[[1]] <- lm(y ~ x2, data = dat_2)

mods_62[[1]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

```{r}
mods_62[[2]] <- lm(y ~ x6, data = dat_2)

mods_62[[2]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

It is reasonable to assume that people tend to have more accompanies as age increases after 18. Also, the following model proves the relationship between `x2` and `x6`. We can say that the effect of `x6` on `y` is mediated by `x2`. With `x6` affecting `y` as well, `x2` does not mediate `x6` completely. So `x2` and `x6` are both supposed to appear in the model for `y`. The mediation factor `x2` may affect `y` though other ways, which will be explored in section 10. Besides, we find that `x6` does not affect `y` in other ways in section 10.

```{r}
mods_62[[3]] <- lm(x2 ~ x6, data = dat_2)

mods_62[[3]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

### 8-2. Causality of `x3`-`x4`

When taking `x3` and `x4` into consideration, the models assocating `y` with `x3` or `x4` both show no significance, though `x3` and `x4` are highly correlated. So neither `x3` nor `x4` should be included in the model.

```{r}
mods_34 <- list()
mods_34[[1]] <- lm(x4 ~ x3, data = dat_2)

mods_34[[1]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

```{r}
mods_34[[2]] <- lm(formula = y ~ x3 + x4, data = dat_2)

mods_34[[2]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```


### 8-3. Causality of `x4`-`x8`

It can be seen that `y` is highly correlated to `x8`.

```{r}
mods_48 <- list()
mods_48[[1]] <-  lm(y ~ x8, data = dat_2)

mods_48[[1]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

Also, `x8` is highly related to `x4`, which makes sense, because people with more income tend to buy houses with more rooms.

```{r}
mods_48[[2]] <-  lm(x8 ~ x4, data = dat_2)

mods_48[[2]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

However, from subsection 9.2 we already know that `y` is not highly correlated with `x4`, which is illustrated again by the following model. So we say that the effect of `x4` on `y` is completely mediated by `x8`. Though it makes sense that people with more income tend to consume more energy on average, the direct effect is completely mediated through `x8` and possibly other factors.

```{r}
mods_48[[3]] <- lm(y ~ x8 + x4, data = dat_2)

mods_48[[3]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

## 9. `mods[[3]]` with extra terms

In `mods[[6]]`, `x2`, `x6` and `x8` are kept at the same time.

```{r}
mods[[6]] <- lm(y ~ x2 + x6 + x8, data = dat_2)

mods[[6]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

```{r}
results_test <- mods[[6]] %>% test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[6]], dat_2, resi2 ~ x2 + x6 + x8 + I(x2^2) +
    I(x6^2) + I(x8^2), 4)) %>%
  bind_rows(test_white(mods[[6]], dat_2, resi2 ~ x2 + x6 + x8 + I(x2^2) +
    I(x6^2) + I(x8^2) + I(x2 * x6) + I(x2 * x8) + I(x6 * x8), 10)) %>%
  bind_rows(test_reset(mods[[6]], dat_2))

results_test %>% tab_ti(F)
```

With the idea of diminishing effects of the number of household members in mind, it is natural to include the square of the household numbers. In addition, `x8` is included according to the discussion in subsection 8-3. Thus, `mods[[7]]` is estimated. Being insignificant, `x6` is excluded, beccause the effect of `x6` on `y` seems to be mediated compeletely by `x2` and `x2^2`.

```{r}
mods[[7]] <- lm(y ~ x2 + x8 + I(x2^2), data = dat_2)
mods[[7]] %>% tab_tidy(T)
```

With number of observations being 299, the model is sensitive to exlusion of some term only if the partial correlation of that term is smaller than 0.01276551. That is, if `p.r.squared` of some term displayed above is smaller than 0.01276551, the hypothesis regaring the coefficient for that term in the likelihood ratio test would not be rejected. The value `0.01276551` is obtained according to the discussion in subsection 7-3. Besides, small `p.value`s indicate that terms cannot be excluded as well. 

```{r, include=F}
lr_x5 <- - 299 * log(1 - 0.01276551)
(1 - pchisq(lr_x5, 1))
```

```{r}
results_test <- mods[[7]] %>% test_jb(dat_2)

results_test %<>%
  bind_rows(test_white(mods[[7]], dat_2, resi2 ~ x2 + x8 + I(x2^2) + I(x8^2),
    3)) %>%
  bind_rows(test_white(mods[[7]], dat_2, resi2 ~ x2 + x8 + I(x2^2) +
    I(x8^2) + I(x2 * x8), 6)) %>%
  bind_rows(test_reset(mods[[7]], dat_2))

results_test %>% tab_ti(F)
```

`mods[[7]]` has a lower AIC than `mods[[6]]` and `mods[[3]]`, so it is choosen as the final model for presentation and conclusion.

```{r}
AIC(mods[[7]], mods[[6]], mods[[3]]) %>% tab_ti(F)
```

## 10. Interpretation of `mods[[7]]`

### 10-1. Model Structure

The overall structure can be illustrated in the following figure. The income is correlated to the number of household memebers, which is probably caused by the correlation between numbers of people and rooms. Because those relations don't contribute to the interpretation of consumptions directly, they are omitted.

![Illustration of the model structure.](./images/1.png)

### 10-2. Regressors

Electricity consumption is highly correlated to the number of household members of the respondent. When there are not too many household members, with more members, the quantity of consumption decreases, probably due to the fact that people tend to share the kitchen. However, the effect diminishes gradually when there are many household members (around 5-8).

```{r}
ti <- tibble(x = seq(1:8), y = 8.77946 + 0.03563 * seq(1:8)^2 - 
  0.52071 * seq(1:8) + 0.07329 * mean(dat$x8))

dat %>%
  mutate(index = row_number()) %>%
  ggplot() +
    geom_boxplot(aes(x2, y, group = cut_width(x2, 1)), outlier.alpha = 0) +
    geom_point(aes(x2, y), shape = 1) +
    geom_text(aes(x2, y, label = ifelse(index == 36 | index == 163 |
      index == 241, as.character(index), "")), hjust=1.5, vjust=0) +
    geom_line(data = ti, mapping = aes(x, y), color = "#F8766D", size = 2)
```

Because `y` is the log of average electricity consumptions, the final relationship between electricity consumptions and numbers of household members and numbers of rooms are:

As for the effect of number of rooms on electricity consumptions, 

```{r}
ti <- tibble(x = seq(2, 22), y = 8.77946 + 0.07329 * seq(2, 22) - 
  0.52071 * mean(dat$x2) + 0.03563 * mean(dat$x2^2))

dat %>%
  mutate(index = row_number()) %>%
  ggplot() +
    geom_boxplot(aes(x8, y, group = cut_width(x8, 1)), outlier.alpha = 0) +
    geom_point(aes(x8, y), shape = 1) +
    geom_line(data = ti, mapping = aes(x, y), color = "#F8766D", size = 2)
```

### 10-3. Orthogonalization of Multiple Regressors

```{r, echo=T}
mods[[8]] <-
  dat_2 %>%
  mutate(z1 = lm(x2 ~ 1)$residuals) %>%
  mutate(z2 = lm(x8 ~ x2 + 1)$residuals) %>%
  mutate(x2.2 = x2^2) %>%
  mutate(z3 = lm(x2.2 ~ x2 + x8 + 1)$residuals) %>%
  {lm(y ~ z1 + z2 + z3, data = .)}
```

```{r}
mods[[8]] -> model  # Print model summary
model$call
model %>% tidy() %>% tab_ti(F)
model %>% glance() %>% tab_ti(F)
rm(model)
```

The intercept in `mods[[8]]` (8.37009) can be interpreted as the exptected value for an individual with average values of `x2`, `x8` and `I(x2^2)`, which can be verified by the prediction using `mods[[7]]`.

```{r, echo=T}
( 8.77946 - 0.52071 * mean(dat_2$x2) + 0.07329 * mean(dat_2$x8) + 
  0.03563 * mean(dat_2$x2^2) ) - 8.37009 <= 1e-4
```

Since the standard errors in `mods[[8]]` are smaller than those in `mods[[7]]`, `mods[[8]]` is used to conduct inference. Particularly, `se` for `Intercept` is reduced by 75.48%, and `se`s for first two regressors are reduced by 71.75% and 2.41%. The estimations for the last term are exactly the same, which is expected.

```{r}
mods[[8]] %>% confint()
```

To reduce average electricity consumptions, people are encouraged to live together in houses with fewer rooms.

<!-- ## 11. One-Sided t-Test of `mods[[1]]` -->

<!-- ```{r, echo=T} -->
<!-- mods[[1]] %>% -->
<!--   summary() %>% -->
<!--   {pt(coef(.)[2, 3], mods[[1]]$df, lower = FALSE)} %>% -->
<!--   {. <= qchisq(0.95, 1, lower.tail = TRUE, log.p = FALSE)} -->
<!-- ``` -->
